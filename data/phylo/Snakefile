import os
import pickle
import pandas as pd
from bgspy.models import BGSModel

PF_DIR = 'phylofit_estimates/'
# MODEL_FILE = '../bmaps/bgspy/bmap_hg38_6x7grid_10000step_10000N_cds_utrs_phastcons_full.pkl'
MODEL_FILE = '../bmaps/bgspy/bmap_hg38_6x7grid_10000step_10000N_cds_utrs_phastcons.pkl'
RESCALE_MODEL_FILE = '../bmaps/bgspy/bmap_rescaled_hg38_6x7grid_10000step_10000N_cds_utrs_phastcons.pkl'
FIT_FILE = '../../fits/hg38_cds_utrs_phastcons_simplex/hg38_cds_utrs_phastcons/window_1000000/mle.pkl'

rule get_knowncanonical_multiz:
  output: "knownCanonical.exonNuc.fa.gz"
  shell:
     """
     wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/multiz20way/alignments/knownCanonical.exonNuc.fa.gz
     """

rule separate_alns:
   # note the "ensembl_ucsc_stable_id.tsv" file was downloaded from biomart website
   input: multiz="knownCanonical.exonNuc.fa.gz", map="ensembl_ucsc_stable_id.tsv"
   output: "cds_alns/dummy.txt"
   shell:
       """
       python ../../tools/split_multiz.py {input.multiz} cds_alns/ {input.map} 
       touch cds_alns/dummy.txt
       """

# not needed, we use whole directory
# if it fails here, run the phylofit command in the README.md 
# (this can't be put in snakemake)
phylo_fits = [os.path.join(PF_DIR, f) for f in os.listdir(PF_DIR)]

rule merge_phylofit:
  input: dummy="phylofit_estimates/dummy.txt", dir=PF_DIR
  output: "phylofit_cds_rates.bed"
  shell:
    """
    python ../../tools/merge_phylofits.py {input.dir} tmp.bed
    sort -k1,1 -k2,2n  tmp.bed > {output[0]}
    """

rule windows:
  input: annot="../annotation/hg38_seqlens.tsv"
  output: "hg38_windows_{width}.bed"
  shell:
    """
    bedtools makewindows -g {input.annot} -w {wildcards.width} | bedtools sort > {output[0]}
    """

rule unscaled_ratchet_data:
  # this is the full raw segment ratchet data, calculated alongside 
  # the B scores.
  # NOTE: this is UNSCALED and should most likely NOT BE USED
  input: model = MODEL_FILE, fit = FIT_FILE
  output: "unscaled_ratchet_data.bed"
  shell: 
     """
     bgspy subrate --bs-file {input.model} --fit {input.fit} --outfile {output[0]}
     """

rule ratchet_rescale_data_split:
  input: model = RESCALE_MODEL_FILE, fit = FIT_FILE
  output: "rescaled_ratchet_data_cds.bed", "rescaled_ratchet_data_utr.bed", "rescaled_ratchet_data_phastcons.bed"
  shell: 
     """
     bgspy subrate --split --bs-file {input.model} --fit {input.fit} --outfile "rescaled_ratchet_data"
     """

rule ratchet_rescale_data:
  input: model = RESCALE_MODEL_FILE, fit = FIT_FILE
  output: "rescaled_ratchet_data.bed"
  shell: 
     """
     bgspy subrate --bs-file {input.model} --fit {input.fit} --outfile {output[0]}
     """

rule unscaled_ratchet_data_cds:
  input: model = MODEL_FILE, fit = FIT_FILE
  output: "unscaled_ratchet_data_cds.bed", "unscaled_ratchet_data_utr.bed", "unscaled_ratchet_data_phastcons.bed"
  shell: 
     """
     bgspy subrate --split --bs-file {input.model} --fit {input.fit} --outfile "unscaled_ratchet_data"
     """

## load and clean up Uricchio et al data
## NOTE: the protein coding stats is processed in data/annotation
rule uricchio_data:
  input: data="41559_2019_890_MOESM3_ESM.txt", cds_stats="../annotation/ensembl_cds_canonical_protein_coding_stats.tsv"
  output: "uricchio_data.tsv"
  run: 
    cols = ('gene_id', 'n_nonsyn', 'nonsyn_daf', 'n_syn', 
        'syn_daf', 'fixed_nonsyn', 'fixed_syn')

    gr = pd.read_csv(input.data,
                     sep='\t', names=cols, comment='#')
    gr['dnds'] = gr['fixed_nonsyn'] / gr['fixed_syn']

    cd = pd.read_csv(input.cds_stats, sep='\t')

    gr = gr.merge(cd, on='gene_id')
    gr['total_fixed'] = gr['fixed_nonsyn']+gr['fixed_syn']
    gr['syn_prop'] = gr['fixed_syn'] / gr['S']
    gr['nonsyn_prop'] = gr['fixed_nonsyn'] / gr['N']
    gr['total_prop'] = gr['total_fixed'] / (gr['S'] + gr['N'])
    gr['dNdS'] = gr['fixed_nonsyn']/gr['fixed_syn']
    gr = gr[~gr['chrom'].isin(['chrX', None])]


    new_cols = ['chrom',  'start', 'end', 'gene_id', 'n_nonsyn', 'n_syn',
                'fixed_nonsyn', 'fixed_syn', 'dnds', 
                'total_fixed', 'len', 'cai', 'gc', 'gc3', 'syn_prop', 
                'nonsyn_prop', 'total_prop', 'dNdS', 'S', 'N']

    gr = gr.sort_values(['chrom', 'start', 'end'])
    gr = gr[new_cols]
    gr = gr.rename(columns={'chrom': '#chrom'})
    gr.to_csv(output[0], sep='\t', index=False)

rule uricchio_merged_raw:
  """
  Merge in the raw ratchet data and the phylofit substitution rates for
  the whole CDS from ensembl_cds_canonical_protein_coding_stats.tsv.

  NOTE: We do not have the precise transcripts Uricchio et al used.
  But this is fine, it's just for validation and dn/ds.
  The stats are on the canonical transcripts CDS FASTA file.

  The phylofit data is from the UCSC Multiz 20-way, but subset for 5 primates.
  The UCSC file I use is knownCanonical.exonNuc.fa.gz  for the phylofit estimates.
  """
  input: data="uricchio_data.tsv", 
         unscaled_ratchet="unscaled_ratchet_data_cds.bed", 
         pf="phylofit_cds_rates.bed", 
         rescaled_ratchet="rescaled_ratchet_data_cds.bed"
  output: "uricchio_data_merged_raw.bed"
  shell:
      """
      bedtools map -a {input.data} -b {input.rescaled_ratchet} -c 6,7 -o mean,sum > tmp.bed
      bedtools map -a tmp.bed -b {input.unscaled_ratchet} -c 6,7 -o mean,sum > tmp2.bed
      bedtools map -a tmp2.bed -b {input.pf} -c 5,6 -o mean,sum > {output[0]}
      rm -f tmp.bed tmp2.bed
      """

rule uricchio_merged:
  input: "uricchio_data_merged_raw.bed"
  output: "uricchio_data_merged.tsv"
  run:
    new_cols = ['chrom',  'start', 'end', 'gene_id', 'n_nonsyn', 'n_syn',
                'fixed_nonsyn', 'fixed_syn', 'dnds', 
                'total_fixed', 'len', 'cai', 'gc', 'gc3', 'syn_prop', 
                'nonsyn_prop', 'total_prop', 'dNdS', 'S', 'N',
                
                'r_rs', 'r_rs_nbases',
                'r_us', 'r_us_nbases',
                'pf_subrate', 'pf_nbases',
                ]
    gr = pd.read_csv(input[0], sep='\t', comment='#', names=new_cols)
    gr['width'] = gr['end'] - gr['start']
    gr['midpoint'] = 0.5*(gr['start'] + gr['end'])
    gr.to_csv(output[0], sep='\t', index=False)


rule phylofit_cds:
  input: pf="phylofit_cds_rates.bed", ratchet="rescaled_ratchet_data_{feature}.bed"
  output: "phylofit_rescaled_ratchet_{feature}_merged.bed"
  shell:
     """
     bedtools map -a {input.pf} -b {input.ratchet} -prec 10 -c 6 -o mean > {output[0]}
     """


rule ratchet_windows:
  input: ratchet="rescaled_ratchet_data_cds.bed", win="hg38_windows_{width}.bed"
  output: "rescaled_ratchet_cds_rates_binned_{width}"
  shell:
    """
    bedtools map -a {input.win} -b {input.ratchet} -c 5,6,7 -o mean,mean,sum > {output}
    """

rescale_window_ratchet = ["rescaled_ratchet_cds_rates_binned_1000000.bed", 
                          "rescaled_ratchet_cds_rates_binned_100000.bed",
                          "rescaled_ratchet_cds_rates_binned_5000000.bed"]


ALL_FILES = ["phylofit_cds_rates.bed",
             "cds_alns/dummy.txt",
             "rescaled_ratchet_data_cds.bed", 
             "uricchio_data.tsv", 
             "uricchio_data_merged_raw.bed", 
             "uricchio_data_merged.tsv",
             "unscaled_ratchet_data_cds.bed", 
             "phylofit_rescaled_ratchet_cds_merged.bed",
             "phylofit_rescaled_ratchet_utr_merged.bed",
             "phylofit_rescaled_ratchet_phastcons_merged.bed",
             *rescale_window_ratchet]

rule all:
  input: ALL_FILES

