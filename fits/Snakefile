"""
Main Snakemake-based model fitting pipeline.
"""
import socket
import numpy as np

## personal stuff for my project (sorry, safe to ignore)
# we scale down the number of cores if we're doing this on the cluster
dev = 'sesame'
is_dev =  socket.gethostname().startswith(dev)


## general stuff 
NSTARTS = 2_000
NSTARTS_LOO = 2_000

# note: ncores_loo is per-job;
# you'll want to account number of total jobs,
# -j
if not is_dev:
    NCORES = 40
    NCORES_LOO = NCORES
    NCORES_B = 40
    NCORES_BP = 20
else:
    # just blast through a bunch without parallel snakemake jobs
    NCORES = 70
    NCORES_LOO = 70
    NCORES_B = 40
    NCORES_BP = 20


CHROMS = [f'chr{x}' for x in range(1, 23)]

# from config file
name = config["name"]
mu = config.get("mu", None)
seqlens_file = config["seqlens_file"]
recmap_file = config["recmap_file"]
access_file = config["access_file"]
features_file = config["features_file"]
neut_file = config["neut_file"]
fasta_file = config["fasta_file"]
counts_dir = config["counts_dir"]
N = config["N"]
fit_rescaled = config["fit_rescaled"]
windows = [int(w) for w in config["windows"]]
blocksizes = config["blocksizes"]
njack_samples = config["num_jackknife_samples"]

## ------ created data
# name auto-generated
Bs_file = f"bmap_6x8grid_100000step_{N}N_{name}.pkl"
Bs_file_rescaled = f"bmap_rescaled_{{window}}_6x8grid_100000step_{N}N_{name}.pkl"

# the data that goes into the likelihood model
model_data = [f'model_data_{w}.pkl' for w in windows]

# the data needed for all fits in this model
loo_results = []
jk_results = []
mle_results = []
mle_results_rescaled = []
boots_results = []
for window in windows:
  mle_results.append(f'fit_{window}/mle.pkl')
  if fit_rescaled:
      mle_results_rescaled.append(f'fit_{window}_rescaled/mle.pkl')
  # chromosome-level LOO for R2, etc
  for chrom in CHROMS:
      loo = f"fit_{window}/loo_chrom/loo_{chrom}.pkl"
      loo_results.append(loo)
  # block-level jackknife
  for blocksize in blocksizes:
      # this is a very close approximation to the block-jackknife, 
      # were samples at evenly spaced fraction positions are taken
      # and left out; this prevents us from having to know the number
      # of blocks a priori, which is easier when pushing computation to the
      # cluster
      # non-inclusive, otherwise index error
      for frac in np.linspace(0, 1, njack_samples)[:-1]:
          frac = np.round(frac, 5)
          jk_results.append(f'fit_{window}/jackknife/{blocksize}/jackknife_{frac}.pkl')
    

## ------- rules
rule bmap:
  input: seqlens_file=seqlens_file,
         recmap=recmap_file,
         features=features_file
  output: pkl_b_file="bmap_{grid_str,[0-9x]+}grid_{step}step_{N}N_{name}.pkl"
  params: ncores=NCORES_B, ncores_bp=NCORES_BP
  shell:
    """
    bgspy calcb \
      --recmap {input.recmap} --annot {input.features} \
      --seqlens {input.seqlens_file} --g '{wildcards.grid_str}' \
      --output {output} --popsize {wildcards.N} \
      --ncores {params.ncores} --ncores-Bp {params.ncores_bp} \
      --step {wildcards.step}
    """

rule save_data:
  # only get the data, e.g. for testing, for this model
  input: seqlens=seqlens_file, recmap_file=recmap_file,
         access_file=access_file, fasta_file=fasta_file,
         neut_file=neut_file, counts_dir=counts_dir,
         bs_file=Bs_file
  output: 'model_data_{window,\d+}.pkl'
  params: name=name
  shell:
    """
    mkdir -p {params.name}
    bgspy data --seqlens {input.seqlens} --recmap {input.recmap_file} \
      --counts-dir {input.counts_dir} --neutral {input.neut_file} \
      --access {input.access_file} --bs-file {input.bs_file} \
      --window {wildcards.window} \
      --fasta {input.fasta_file} \
      --output {output[0]}
    """

rule fit:
  input: 'model_data_{window}.pkl'
  output: 'fit_{window,\d+}/mle.pkl'
  params: ncores=NCORES, nstarts=NSTARTS, mu=mu, name=name
  benchmark: "benchmarks/fit_{window}.txt"
  shell:
    """
    mkdir -p {params.name}
    bgspy fit \
      --data {input[0]} \
      --ncores {params.ncores} --nstarts {params.nstarts} \
      --mu {params.mu} \
      --output {output[0]}
    """

rule block_jackknife:
  # note that this doesn't actually require fit object, but 
  # I prefer that gets calculated first
  input: data='model_data_{window}.pkl', fit='fit_{window}/mle.pkl'
  output: 'fit_{window,\d+}/jackknife/{blocksize}/jackknife_{frac}.pkl'
  params: ncores=NCORES_LOO, nstarts=NSTARTS_LOO
  shell:
    """
    bgspy jackblock \
      --blockwidth {wildcards.blocksize} \
      --data {input.data} \
      --output {output[0]} \
      --blockfrac {wildcards.frac} \
      --ncores {params.ncores} --nstarts {params.nstarts}
    """


rule loo_chrom:
  # note that this doesn't actually require fit object, but 
  # I prefer that gets calculated first
  input: data='model_data_{window}.pkl', fit='fit_{window}/mle.pkl'
  output: 'fit_{window,\d+}/loo_chrom/loo_{chrom}.pkl'
  params: ncores=NCORES_LOO, nstarts=NSTARTS_LOO
  shell:
    """
    bgspy jackchrom \
      --data {input.data} \
      --output {output[0]} \
      --chrom {wildcards.chrom} \
      --ncores {params.ncores} --nstarts {params.nstarts}
    """

# ----- locally-rescaled fits -----
rule bmap_rescale:
  input: seqlens_file=seqlens_file,
         recmap=recmap_file,
         features=features_file,
         fit='fit_{window}/mle.pkl',
         b=Bs_file
  output: pkl_b_file="bmap_rescaled_{window,\d+}_{grid_str,[0-9x]+}grid_{step}step_{N}N_{name}.pkl"
  params: ncores=NCORES_B, ncores_bp=NCORES_BP
  shell:
    """
    bgspy calcb \
      --recmap {input.recmap} --annot {input.features} \
      --seqlens {input.seqlens_file} --g '{wildcards.grid_str}' \
      --output {output} --popsize {wildcards.N} \
      --ncores {params.ncores} --ncores-Bp {params.ncores_bp} \
      --step {wildcards.step} \
      --rescale-fit {input.fit} --only-Bp \
      --rescale-Bp-file {input.b}
    """

rule save_data_rescale:
  input: seqlens=seqlens_file, recmap_file=recmap_file,
         access_file=access_file, fasta_file=fasta_file,
         neut_file=neut_file, counts_dir=counts_dir,
         bs_file=Bs_file_rescaled
  output: 'model_data_{window,\d+}_rescaled.pkl'
  params: name=name
  shell:
    """
    mkdir -p {params.name}
    bgspy data --seqlens {input.seqlens} --recmap {input.recmap_file} \
      --counts-dir {input.counts_dir} --neutral {input.neut_file} \
      --access {input.access_file} --bs-file {input.bs_file} \
      --window {wildcards.window} \
      --fasta {input.fasta_file} \
      --output {output[0]}
    """

rule fit_rescaled:
  input: 'model_data_{window,\d+}_rescaled.pkl'
  output: 'fit_{window,\d+}_rescaled/mle.pkl'
  params: ncores=NCORES, nstarts=NSTARTS, mu=mu, name=name
  benchmark: "benchmarks/fit_{window}_rescaled.txt"
  shell:
    """
    mkdir -p {params.name}
    bgspy fit \
      --data {input[0]} \
      --ncores {params.ncores} --nstarts {params.nstarts} \
      --output {output[0]} --only-Bp
    """

## ----- CLI rules 
rule data:
  input: model_data

rule mle:
  input: mle_results

rule jackknife:
  input: mle_results, jk_results

rule loo:
  input: mle_results, loo_results

rule rescaled:
  input: mle_results_rescaled

rule all:
  input: model_data, mle_results, loo_results, jk_results, mle_results_rescaled


